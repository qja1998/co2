{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Transformer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "np.random.seed(0)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "random.seed(0)\n",
    "\n",
    "if torch.cuda.is_available(): device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqja1998\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\qja19\\Desktop\\co2\\machine_learning\\wandb\\run-20230322_174603-pvkm4ksc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/qja1998/co2-machine_learning/runs/pvkm4ksc' target=\"_blank\">dashing-sea-1</a></strong> to <a href='https://wandb.ai/qja1998/co2-machine_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/qja1998/co2-machine_learning' target=\"_blank\">https://wandb.ai/qja1998/co2-machine_learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/qja1998/co2-machine_learning/runs/pvkm4ksc' target=\"_blank\">https://wandb.ai/qja1998/co2-machine_learning/runs/pvkm4ksc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init()\n",
    "wandb.run.name = 'transformer'\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GasDataset(Dataset):\n",
    "    def __init__(self, df, seq_len, out_len, year=None):\n",
    "        self.seq_len = seq_len\n",
    "        self.out_len = out_len\n",
    "\n",
    "        self.df = df\n",
    "        if year is not None:\n",
    "            self.df = self.df.loc[self.df['year'] == year]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df) - self.seq_len - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_end = idx + self.seq_len\n",
    "        output_end = input_end + self.out_len\n",
    "\n",
    "        x_data = self.df.iloc[idx:input_end]\n",
    "        y_data = self.df.iloc[input_end:output_end]\n",
    "\n",
    "        date_x, x = x_data.date, torch.from_numpy(np.array(x_data[['supply']], dtype=np.float32))\n",
    "        date_y, y = y_data.date, torch.from_numpy(np.array(y_data[['supply']], dtype=np.float32))\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self,d_model, seq_len, out_len, nhead, nhid, nlayers, model_type, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        if model_type == \"enc-dec\":\n",
    "            self.embedding = nn.Linear(1, d_model)\n",
    "            self.transformer = Transformer(d_model=d_model, nhead=nhead, dim_feedforward=nhid, num_encoder_layers=nlayers, num_decoder_layers=nlayers,dropout=dropout)\n",
    "            self.linear = nn.Linear(d_model, 1)\n",
    "        \n",
    "        else:\n",
    "            if model_type == \"enc\":\n",
    "                encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "                self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "\n",
    "            elif model_type == \"dec\":\n",
    "                encoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "                self.transformer_encoder = nn.TransformerDecoder(encoder_layer, num_layers=nlayers)\n",
    "            \n",
    "            self.embedding = nn.Sequential(\n",
    "                nn.Linear(1, d_model//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model//2, d_model)\n",
    "            )\n",
    "            \n",
    "            self.fc =  nn.Sequential(\n",
    "                nn.Linear(d_model, d_model//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(d_model//2, 1),\n",
    "            )\n",
    "\n",
    "            self.fc2 = nn.Sequential(\n",
    "                nn.Linear(seq_len, (seq_len + out_len)//2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear((seq_len + out_len)//2, out_len)\n",
    "            )\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, srcmask, tgtmask):\n",
    "        if self.model_type == \"enc-dec\":\n",
    "            src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "            src = self.pos_encoder(src)\n",
    "            tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "            tgt = self.pos_encoder(tgt)\n",
    "\n",
    "            output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask)\n",
    "            output = self.linear(output)\n",
    "\n",
    "        else:\n",
    "            src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "            src = self.pos_encoder(src)\n",
    "            output = self.transformer_encoder(src.transpose(0,1), srcmask).transpose(0,1)\n",
    "            output = self.fc(output)[:,:,0]\n",
    "            output = self.fc2(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def gen_attention_mask(x):\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 14, 1]) torch.Size([64, 7, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m     src_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: transpose() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int dim0, int dim1)\n * (name dim0, name dim1)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 14\n",
    "out_len = 7\n",
    "train_rate = 0.8\n",
    "df = pd.read_csv(\"data/korea/kor_gas_day.csv\")\n",
    "df = df.loc[df['type'] == 'A']\n",
    "train_len = int(len(df) * train_rate)\n",
    "\n",
    "df_train, df_val = df.iloc[:train_len], df.iloc[train_len:]\n",
    "train_dataset = GasDataset(df_train, seq_len, out_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64)\n",
    "\n",
    "lr = 1e-3\n",
    "model = TransformerModel(256, 14, 7, 8, 256, 2, 'enc-dec', 0.1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (x, y) in enumerate(train_loader):\n",
    "    print(x.shape, y.shape)\n",
    "    x = x.transpose((2, 0, 1)).to(device)\n",
    "    y = y.transpose((2, 0, 1)).to(device)\n",
    "    src_mask = model.generate_square_subsequent_mask(x.shape[1]).to(device)\n",
    "    tgt_mask = model.generate_square_subsequent_mask(y.shape[1]).to(device)\n",
    "    print(x.shape, y.shape, src_mask.shape, tgt_mask.shape)\n",
    "    output = model(x, y, src_mask, tgt_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_data, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for i in tqdm(range(epoch)):\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x = x.squeeze(-1).to(device)\n",
    "            y = y.unsqueeze(-1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            wandb.log({\n",
    "                \"epoch\": i,\n",
    "                \"Loss\": loss,\n",
    "                'x': x,\n",
    "                'y': y\n",
    "            })\n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, i, epoch,\n",
    "                100. * i / epoch, loss.item()))\n",
    "\n",
    "    if val_data is not None:\n",
    "        model.eval()\n",
    "        loss = 0\n",
    "        true_val = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (x, y) in enumerate(val_data):\n",
    "                \n",
    "                x = x.view(1, -1).to(device)\n",
    "                y = y.unsqueeze(-1).to(device)\n",
    "                output = model(x).squeeze(-1)\n",
    "\n",
    "                loss += criterion(output, y)\n",
    "                true_val.append(float(y.cpu().numpy()))\n",
    "                predictions.append(float(output.cpu().numpy()))\n",
    "        \n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}'.format(loss / len(val_data)))\n",
    "        \n",
    "        plt.figure(figsize=(30,15))\n",
    "        x = np.arange(len(true_val))\n",
    "        plt.subplot(411)\n",
    "        plt.plot(x, true_val, label='true', c='blue')\n",
    "        plt.plot(x, predictions, label='predictions', c='red')\n",
    "        plt.legend()\n",
    "\n",
    "        true_val_cumsum = np.cumsum(true_val)\n",
    "\n",
    "        tmp1 = predictions[:]\n",
    "        for i, v in enumerate(true_val_cumsum[:-1]):\n",
    "            tmp1[i + 1] = v + tmp1[i + 1]\n",
    "            \n",
    "        plt.subplot(412)\n",
    "        plt.plot(x, true_val_cumsum, label='true', c='blue')\n",
    "        plt.plot(x, predictions, label='predictions', c='red')\n",
    "        plt.plot(x, np.cumsum(tmp1), label='self cumsum', c='orange')\n",
    "        plt.legend()\n",
    "\n",
    "        self_predictions = []\n",
    "        x = val_data[0][0].view(1, -1).to(device)\n",
    "        y = val_data[0][1].unsqueeze(-1).to(device)\n",
    "        with torch.no_grad():\n",
    "            for i, (_, true_y) in enumerate(val_data):\n",
    "                output = model(x).squeeze(-1)\n",
    "                tmp = x[0][1:].unsqueeze(0)\n",
    "                x = torch.cat([tmp, output.unsqueeze(0)], dim=1)\n",
    "                y = output\n",
    "\n",
    "                loss += criterion(output, true_y.unsqueeze(-1).to(device))\n",
    "                self_predictions.append(float(output.cpu().numpy()))\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}'.format(loss / len(val_data)))\n",
    "        x = np.arange(len(true_val))\n",
    "        plt.subplot(413)\n",
    "        plt.plot(x, true_val, label='true', c='blue')\n",
    "        plt.plot(x, self_predictions, label='predictions', c='red')\n",
    "        plt.legend()\n",
    "        \n",
    "        tmp2 = self_predictions[:]\n",
    "        for i, v in enumerate(true_val_cumsum[:-1]):\n",
    "            tmp2[i + 1] = v + tmp2[i + 1]\n",
    "        \n",
    "        plt.subplot(414)\n",
    "        plt.plot(x, true_val_cumsum, label='true', c='blue')\n",
    "        plt.plot(x, self_predictions, label='predictions', c='red')\n",
    "        plt.plot(x, np.cumsum(tmp2), label='self cumsum', c='orange')\n",
    "        plt.legend()\n",
    "\n",
    "        return true_val, true_val_cumsum, predictions, self_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GasDataset.__init__() missing 1 required positional argument: 'out_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m train_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m*\u001b[39m train_rate)\n\u001b[0;32m      9\u001b[0m df_train, df_val \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:train_len], df\u001b[38;5;241m.\u001b[39miloc[train_len:]\n\u001b[1;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGasDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m GasDataset(df_val, seq_len)\n",
      "\u001b[1;31mTypeError\u001b[0m: GasDataset.__init__() missing 1 required positional argument: 'out_len'"
     ]
    }
   ],
   "source": [
    "seq_len = 21\n",
    "out\n",
    "train_rate = 0.8\n",
    "df = pd.read_csv(\"data/korea/kor_gas_day.csv\")\n",
    "df = df.loc[df['type'] == 'A']\n",
    "df.loc[:, ['supply']] = df.loc[:, ['supply']].astype(float).diff()\n",
    "df = df[1:]\n",
    "train_len = int(len(df) * train_rate)\n",
    "\n",
    "df_train, df_val = df.iloc[:train_len], df.iloc[train_len:]\n",
    "train_dataset = GasDataset(df_train, seq_len)\n",
    "val_dataset = GasDataset(df_val, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm'\n",
    "hidden_szie = 128\n",
    "num_layers = 3\n",
    "learning_rate = 1e-3\n",
    "conv = False\n",
    "\n",
    "epochs = 500\n",
    "batch_szie = 512\n",
    "\n",
    "model = Model(model_name, seq_len, hidden_szie, num_layers, conv).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_szie, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_szie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train(model, train_loader, val_dataset, optimizer, criterion, epochs)\n",
    "true_val, true_val_cumsum, predictions, self_predictions = results[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "fe56894a06ef2437ff1ea5593c7ec177d92d09261e0a0125f5b53250947cba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
